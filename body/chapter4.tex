\section{改进的U-Net模型设计}

尽管U-Net网络的跳跃连接能够有效地结合浅层和深层特征，但在处理小目标时仍可能因为上下文信息的缺失而导致定位不准确。此外，背景噪声的干扰使得无关区域的特征也会参与到网络的学习过程中，进而影响模型的性能。为了克服U-Net网络存在的这些问题，本研究提出了改进的U-Net模型设计，本章内容将对其进行具体阐述。

\subsection{网络的改进设计}

本研究引入了注意力机制来改进U-Net网络的结构，注意力机制的核心思想是通过动态地调整特征图的权重，自动聚焦于目标区域，抑制无关背景的干扰。这种方法能够增强模型对重要区域的敏感度，提高模型对小目标的定位能力。

为了实现这一目标，本研究设计在跳跃连接处插入注意力门，动态地筛选并加权输入特征图。具体而言，注意力门根据输入特征图和从粗尺度上提取的上下文信息，计算每个像素的注意力系数，并利用该系数对特征图进行加权，从而只保留对分割任务有用的区域。这一策略无需额外的外部局部化模型，通过自动学习重要区域，有效提升了模型的分割精度，同时避免了传统多阶段模型中冗余计算和参数过多的问题。

此外，注意力模块采用的网格注意力机制（Grid Attention Mechanism），相较于传统通道注意力（如SENet）仅关注通道维度，本研究的网格注意力机制通过局部区域的动态调整，兼顾空间与通道信息，更适用于医学图像中目标形态多变的场景。

\subsection{注意力门模块的结构与原理}

注意力门模块的内部结构主要由三个关键部分组成：输入特征的加权过程、注意力系数的计算以及门控机制的输出。具体而言，该模块包括权重矩阵 $W_x$ 和 $W_g$、非线性激活函数 ReLU、Sigmoid 激活函数以及注意力权重的计算等组成部分。

首先，注意力门模块接收来自U-Net编码器的特征图 $x_l$ 和解码器的门控信号 $g_l$ 作为输入。$x_l$ 为编码器第 $l$ 层输出的特征图，$g_l$ 则是来自解码器的特征图，它为后续的注意力加权提供上下文信息。为了将这两者的特征进行融合，模块采用了两种1×1卷积操作：一个用于处理编码器的特征图 $x_l$，另一个用于处理解码器的门控信号 $g_l$。这两者都通过对应的卷积核 $W_x$ 和 $W_g$ 进行映射到一个中间空间，从而保持信息的一致性并准备后续的注意力计算。

接下来，在这两路特征图被处理后，它们通过ReLU激活函数进行非线性转换，这一步骤有助于引入非线性特征表达，使得网络能够更好地学习到复杂的关系。此时，经过ReLU激活的特征图被送入一个加法操作，进行特征融合，得到一个结合了编码器和解码器信息的中间表示。为了进一步处理该表示并生成最终的注意力权重，模块通过Sigmoid激活函数计算得到一个归一化的注意力系数 $\alpha_l$，该系数决定了每个像素的权重。最后，这些权重通过逐元素相乘的方式与编码器特征图 $x_l$ 进行加权，从而生成加权后的特征图 $\tilde{x}_l = \alpha_l \cdot x_l$。

在计算过程中，注意力系数 $\alpha_l$ 的计算公式如下：

\begin{equation}
    q_{\text{att}}^l = \psi^T \left( \sigma_1 (W_x^T x_l + W_g^T g_l + b_g) \right) + b_\psi
\end{equation}

\begin{equation}
    \alpha_l = \sigma_2 \left( q_{\text{att}}^l(x_l, g_l; \Theta_{\text{att}}) \right)
\end{equation}

其中，$\sigma_1$ 和 $\sigma_2$ 分别为 ReLU 和 Sigmoid 激活函数，$W_x$ 和 $W_g$ 是用于特征映射的权重矩阵，$\psi$ 是线性变换矩阵，$b_g$ 和 $b_\psi$ 是偏置项。在这个公式中，$q_{\text{att}}^l$ 是通过加法操作和线性变换后得到的注意力得分，$\alpha_l$ 是最终计算得到的注意力系数。通过Sigmoid激活，$\alpha_l$ 的值范围被限制在$[0, 1]$之间，能够根据不同位置的特征重要性动态调整每个位置的注意力权重。


\subsection{具体实现与训练策略}

在网络结构中的嵌入位置上，注意力模块被集成到U-Net的跳跃连接部分。传统的U-Net通过将编码器的低层特征图与解码器高层特征图进行拼接来进行信息传递，而在引入注意力机制后，注意力模块被放置在编码器和解码器跳跃连接的中间。在这一位置，注意力模块可以根据解码器的上下文信息来加权编码器的特征图，确保网络关注到最重要的区域并忽略无关背景。具体而言，在每个跳跃连接处，编码器的特征图 $x_l$ 和解码器的特征图 $g_l$ 会被送入注意力门模块，经过加权后再与解码器的上采样特征进行拼接。这一设计使得注意力机制能够在局部区域进行动态学习，从而提高了分割的精度，尤其是在分割小目标和复杂背景时的表现。

在工程设计上，本研究引入注意力机制改进的U-Net进行了若干关键实现选择，以提高计算效率和模型的泛化能力。首先，在U-Net的解码器中，我们选择使用双线性插值代替转置卷积进行上采样。双线性插值是一种常见的图像插值方法，它通过对相邻像素进行加权平均来生成上采样后的图像，这样可以有效避免转置卷积中可能出现的伪影问题，并且计算开销较小。

此外，网络中的BatchNorm（批归一化）操作并未在所有层次中都加入，而是根据需要灵活使用。批归一化通过对每一层的输入进行标准化处理，能够加速网络的收敛并减小过拟合的风险。在改进的U-Net中，BatchNorm的使用主要集中在卷积层之后的部分，以保证特征图的稳定性并加速训练过程。